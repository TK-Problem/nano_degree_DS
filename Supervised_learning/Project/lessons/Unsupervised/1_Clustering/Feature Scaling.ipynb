{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "With any distance based machine learning model (regularized regression methods, neural networks, and now kmeans), you will want to scale your data.  \n",
    "\n",
    "If you have some features that are on completely different scales, this can greatly impact the clusters you get when using K-Means. \n",
    "\n",
    "In this notebook, you will get to see this first hand.  To begin, let's read in the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as p\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "import helpers2 as h\n",
    "import tests as t\n",
    "\n",
    "\n",
    "# Create the dataset for the notebook\n",
    "data = h.simulate_data(200, 2, 4)\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['height', 'weight']\n",
    "df['height'] = np.abs(df['height']*100)\n",
    "df['weight'] = df['weight'] + np.random.normal(50, 10, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Next, take a look at the data to get familiar with it.  The dataset has two columns, and it is stored in the **df** variable.  It might be useful to get an idea of the spread in the current data, as well as a visual of the points.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell if you would like as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a dataset, let's look at some options for scaling the data.  As well as how the data might be scaled.  There are two very common types of feature scaling that we should discuss:\n",
    "\n",
    "\n",
    "**I.  MinMaxScaler**\n",
    "\n",
    "In some cases it is useful to think of your data in terms of the percent they are as compared to the maximum value.  In these cases, you will want to use **MinMaxScaler**.\n",
    "\n",
    "**II. StandardScaler**\n",
    "\n",
    "Another very popular type of scaling is to scale data so that it has mean 0 and variance 1.  In these cases, you will want to use **StandardScaler**.  \n",
    "\n",
    "It is probably more appropriate with this data to use **StandardScaler**.  However, to get practice with feature scaling methods in python, we will perform both.\n",
    "\n",
    "`2.` First let's fit the **StandardScaler** transformation to this dataset.  I will do this one so you can see how to apply preprocessing in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ss = p.StandardScaler().fit_transform(df) # Fit and transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss = pd.DataFrame(df_ss) #create a dataframe\n",
    "df_ss.columns = ['height', 'weight'] #add column names again\n",
    "\n",
    "plt.scatter(df_ss['height'], df_ss['weight']); # create a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now it's your turn.  Try fitting the **MinMaxScaler** transformation to this dataset. You should be able to use the previous example to assist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe\n",
    "#change the column names\n",
    "#plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.`  Now let's take a look at how kmeans divides the dataset into different groups for each of the different scalings of the data.  Did you end up with different clusters when the data was scaled differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_kmeans(data, centers):\n",
    "    '''\n",
    "    INPUT:\n",
    "        data = the dataset you would like to fit kmeans to (dataframe)\n",
    "        centers = the number of centroids (int)\n",
    "    OUTPUT:\n",
    "        labels - the labels for each datapoint to which group it belongs (nparray)\n",
    "    \n",
    "    '''\n",
    "    kmeans = KMeans(centers)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    return labels\n",
    "\n",
    "labels = fit_kmeans(df_mm, 10) #fit kmeans to get the labels\n",
    "    \n",
    "# Plot the original data with clusters\n",
    "plt.scatter(df['height'], df['weight'], c=labels, cmap='Set1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot each of the scaled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another plot of the other scaled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your response here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
